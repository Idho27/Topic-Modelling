{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Laptop\n",
      "[nltk_data]     Ku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk, unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import errno\n",
    "import pandas as pd\n",
    "from sklearn import naive_bayes, metrics, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someth weird happen recent clean room closet door open even though lock\n"
     ]
    }
   ],
   "source": [
    "def casefolding(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def word_token(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def stemming (tokens):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    result = list()\n",
    "    for word in tokens:\n",
    "        result.append(porter_stemmer.stem(word))\n",
    "        \n",
    "    return ' '.join(result)\n",
    "    return text\n",
    "\n",
    "def stopword (tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_text = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            clean_text.append(word)\n",
    "    return clean_text\n",
    "\n",
    "def noise_rem (text):\n",
    "    text = re.sub('[^\\w\\s]','', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def bersih2 (baca):\n",
    "    baca = casefolding(baca)\n",
    "    baca = noise_rem(baca)\n",
    "    tok = word_token(baca)\n",
    "    tok = stopword(tok)\n",
    "    tok = stemming(tok)\n",
    "    baca = ''.join(tok)\n",
    "    \n",
    "    return(baca)\n",
    "\n",
    "a = (\"there is something weird that happened recently while i was cleaning my room. the closet's door was opened by itself even though i lock it\")\n",
    "b = bersih2(a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 14.497674226760864 seconds ---\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "        \n",
    "def masukdata (path):\n",
    "    start_time = time.time()\n",
    "    files = glob.glob(path)\n",
    "    texts = []\n",
    "    workbook = xlsxwriter.Workbook('clean-data.xlsx')\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    row = 0\n",
    "    col = 0\n",
    "\n",
    "    rowHeaders = ['text']\n",
    "    worksheet.write_row(row, col,  tuple(rowHeaders))\n",
    "    for review in files:\n",
    "        f = open(review,'r', encoding='utf-8')\n",
    "        baca = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        baca = bersih2(baca)\n",
    "        rowValues = [baca]\n",
    "        row += 1\n",
    "        worksheet.write_row(row, col, tuple(rowValues))\n",
    "\n",
    "    workbook.close()\n",
    "#         texts.append(baca)\n",
    "#     trainDF = pd.DataFrame()\n",
    "#     trainDF['text'] = texts\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "sets = masukdata('./unsup/*.txt')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   text\n",
      "0     admit great major film releas say 1933 dozen m...\n",
      "1     say name like zombiegeddon atom bomb front cov...\n",
      "2     want read spoiler read thank youbr br ill hone...\n",
      "3     see movi expect budget bad good zombi flick sa...\n",
      "4     bad bone sicknessbr br well yeah isbr br movi ...\n",
      "5     movi isnt worth much time total crap rent movi...\n",
      "6     fan zombiemovi one disappoint movi without goo...\n",
      "7     okay feel must prais given film bad rap deserv...\n",
      "8     film geek could get anyth piec garabag meant s...\n",
      "9     marvel movi start finishbr br meet robert mont...\n",
      "10    base broadway play anoth languag one gem yeste...\n",
      "11    movi must rereleas whole gener comedi fan miss...\n",
      "12    anyon interest famili relationship sure enjoy ...\n",
      "13    kati tippel similar turkish delight film ton n...\n",
      "14    movi easi see director made holland land mains...\n",
      "15    record movi week ago local commun televis stat...\n",
      "16    like paul verhoeven later work robocop total r...\n",
      "17    recent given anchor bay dvd releas titl presen...\n",
      "18    keetj tippel one verhoeven lesser known movi r...\n",
      "19    came across film titl hot sweat local video st...\n",
      "20    one tame yawn nunsploitationbr br like coupla ...\n",
      "21    movi true form open racial prejudic toward nun...\n",
      "22    saw coupl time late seventi parent subscrib ne...\n",
      "23    there bit unintend notorieti connect titl film...\n",
      "24    havent watch mani nunsploit film one among pro...\n",
      "25    even though ambigu estim film whole say sure s...\n",
      "26    one faith nun convent sister maria find obsess...\n",
      "27    mexico almost 100 cathol countri produc sever ...\n",
      "28    movi misguid sinc brainless action flick loren...\n",
      "29    terrif film basic blood gut lorenzo llama stor...\n",
      "...                                                 ...\n",
      "3556  despit catch film halfway time shannon alreadi...\n",
      "3557  mostli decent perform shannen doherti colm feo...\n",
      "3558  think film confus bad plotshannen ok well norm...\n",
      "3559  realli intrigu stori sat glu tv good smart twi...\n",
      "3560  know expect watch movi tell left guess begin e...\n",
      "3561  remov kirk douglass aspect cast essenti enjoy ...\n",
      "3562  rate 10 affect first saw televis year ago prob...\n",
      "3563  quit slow pace facetofac brynnerdouglasi found...\n",
      "3564  jule vern wrote 80 novel well play short stori...\n",
      "3565  surpris jule vern would write stori even surpr...\n",
      "3566  light edg world mark kirk douglass second film...\n",
      "3567  overlook slow move action time hokey plot dial...\n",
      "3568  one favorit film alltim unusu pirat depict mov...\n",
      "3569  view film tri understand famili would tri welc...\n",
      "3570  fast transfer smash wwii bway play fascin unin...\n",
      "3571  film got rather high score leonard maltin guid...\n",
      "3572  spoiler come frame home rural usa taken member...\n",
      "3573  though 24 year sinc saw still rememb think wal...\n",
      "3574  saw movi found extrem puzzl child emil least s...\n",
      "3575  entertain interest film much doesnt say doesbr...\n",
      "3576  tomorrow world play pictur base popular broadw...\n",
      "3577  one must make allow wartim propaganda unit art...\n",
      "3578  tomorrow world thought provok film american fa...\n",
      "3579  heard film saw turner movi classic schedul see...\n",
      "3580  caught bad boy show tulsa oklahoma theater tit...\n",
      "3581  saw pittsburgh pa small theater movi lot fun t...\n",
      "3582  got screener film coupl week ago watch check s...\n",
      "3583  begin see member troma team compani uwe boll s...\n",
      "3584  movi incred ever sinc saw 1977 told wife see r...\n",
      "3585  tcm came acquir wonder silent comedydrama tele...\n",
      "\n",
      "[3586 rows x 1 columns]\n",
      "3586\n"
     ]
    }
   ],
   "source": [
    "fo = pd.ExcelFile('clean-data.xlsx')\n",
    "df = pd.read_excel(fo, 'Sheet1')\n",
    "\n",
    "text = df['text']\n",
    "print(type(df))\n",
    "print(df)\n",
    "text_list =  [i.split() for i in text]\n",
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "#Create Biagram & Trigram Models \n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "bigram = Phrases(text_list, min_count=10)\n",
    "trigram = Phrases(bigram[text_list], min_count=10)\n",
    "\n",
    "for idx in range(len(text_list)):\n",
    "    for token in bigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_list[idx].append(token)\n",
    "    for token in trigram[text_list[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            text_list[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(7873 unique tokens: ['1933', 'actor', 'admit', 'appreci', 'apprehens']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586\n",
      "[(33, 1), (67, 1), (112, 1), (217, 1), (345, 1), (505, 1), (652, 1), (836, 1), (879, 1), (900, 1), (1419, 3), (1616, 1), (1717, 3), (1857, 1), (2503, 2), (2508, 2), (2509, 3), (2612, 1), (2613, 1), (2614, 1), (2615, 1), (2616, 1), (2617, 1), (2618, 1)]\n"
     ]
    }
   ],
   "source": [
    "#build corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
    "\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[100])\n",
    "\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "#function to compute coherence values\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step, mdl):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        if(mdl == 0):\n",
    "            model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "        else:\n",
    "            model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, \n",
    "                             onepass=False, power_iters=150)\n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA done\n",
      "--- 187.1713547706604 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start=2\n",
    "limit=22\n",
    "step=2\n",
    "\n",
    "start_time = time.time()\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
    "                                                        texts=text_list, start=start, limit=limit, step=step,mdl = 0)\n",
    "print(\"LDA done\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "model_list1, coherence_values1 = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
    "                                                        texts=text_list, start=start, limit=limit, step=step, mdl=1)\n",
    "print(\"LSI done\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#show graphs\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"LDA model\")\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(\"LSI model\")\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values1)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model\n",
      "Num Topics = 2  has Coherence Value of 0.2853\n",
      "Num Topics = 4  has Coherence Value of 0.2949\n",
      "Num Topics = 6  has Coherence Value of 0.3935\n",
      "Num Topics = 8  has Coherence Value of 0.404\n",
      "Num Topics = 10  has Coherence Value of 0.3949\n",
      "Num Topics = 12  has Coherence Value of 0.4358\n",
      "Num Topics = 14  has Coherence Value of 0.4719\n",
      "Num Topics = 16  has Coherence Value of 0.4568\n",
      "Num Topics = 18  has Coherence Value of 0.5326\n",
      "Num Topics = 20  has Coherence Value of 0.5032\n",
      "LSI model\n",
      "Num Topics = 2  has Coherence Value of 0.349\n",
      "Num Topics = 4  has Coherence Value of 0.3031\n",
      "Num Topics = 6  has Coherence Value of 0.3745\n",
      "Num Topics = 8  has Coherence Value of 0.3697\n",
      "Num Topics = 10  has Coherence Value of 0.338\n",
      "Num Topics = 12  has Coherence Value of 0.3389\n",
      "Num Topics = 14  has Coherence Value of 0.3291\n",
      "Num Topics = 16  has Coherence Value of 0.3378\n",
      "Num Topics = 18  has Coherence Value of 0.3591\n",
      "Num Topics = 20  has Coherence Value of 0.3663\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "print ('LDA model')\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    \n",
    "print ('LSI model')\n",
    "for m, cv in zip(x, coherence_values1):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "[(0,\n",
      "  '0.006*\"bill_murray\" + 0.003*\"juic\" + 0.003*\"ava_gardner\" + '\n",
      "  '0.003*\"comic_relief\" + 0.003*\"soap_opera\" + 0.002*\"pull_punch\" + '\n",
      "  '0.002*\"one_best\" + 0.002*\"jen\" + 0.002*\"murray\" + 0.002*\"comput_gener\"'),\n",
      " (1,\n",
      "  '0.004*\"hoffman\" + 0.002*\"make_sens\" + 0.002*\"etc_etc\" + '\n",
      "  '0.002*\"agatha_christi\" + 0.002*\"huge_fan\" + 0.002*\"havent_seen\" + '\n",
      "  '0.002*\"21st_centuri\" + 0.002*\"ever_made\" + 0.002*\"timebr_br\" + '\n",
      "  '0.002*\"ever_sinc\"'),\n",
      " (2,\n",
      "  '0.005*\"we_anderson\" + 0.005*\"sherlock_holm\" + 0.004*\"christma\" + '\n",
      "  '0.003*\"sherlock\" + 0.003*\"walk_around\" + 0.002*\"cassidi\" + '\n",
      "  '0.002*\"even_though\" + 0.002*\"quentin_tarantino\" + 0.002*\"church\" + '\n",
      "  '0.002*\"releas_dvd\"'),\n",
      " (3,\n",
      "  '0.004*\"stori_line\" + 0.003*\"special_effect\" + 0.003*\"realli_enjoy\" + '\n",
      "  '0.003*\"year_ago\" + 0.003*\"ginger\" + 0.003*\"dragon\" + 0.003*\"ginger_roger\" + '\n",
      "  '0.002*\"everi_time\" + 0.002*\"havent_seen\" + 0.002*\"fight_club\"'),\n",
      " (4,\n",
      "  '0.003*\"worth_look\" + 0.003*\"old_man\" + 0.003*\"serial_killer\" + '\n",
      "  '0.003*\"end_credit\" + 0.002*\"deniro\" + 0.002*\"would_recommend\" + '\n",
      "  '0.002*\"low_budget\" + 0.002*\"special_effect\" + 0.002*\"ive_ever\" + '\n",
      "  '0.002*\"much_better\"'),\n",
      " (5,\n",
      "  '0.003*\"tv_show\" + 0.003*\"1_10\" + 0.002*\"dont_worri\" + 0.002*\"modern_day\" + '\n",
      "  '0.002*\"well_known\" + 0.002*\"hide_creep\" + 0.002*\"fast_pace\" + '\n",
      "  '0.002*\"look_like\" + 0.002*\"real_life\" + 0.002*\"im_glad\"'),\n",
      " (6,\n",
      "  '0.005*\"robert_de\" + 0.004*\"moustach\" + 0.003*\"lenni\" + 0.003*\"tunnel\" + '\n",
      "  '0.003*\"writer_director\" + 0.003*\"dustin\" + 0.002*\"highli_recommend\" + '\n",
      "  '0.002*\"read_book\" + 0.002*\"lenni_bruce\" + 0.002*\"book\"'),\n",
      " (7,\n",
      "  '0.006*\"indiana_jone\" + 0.005*\"joe_dant\" + 0.004*\"must_see\" + 0.003*\"tunnel\" '\n",
      "  '+ 0.003*\"great_job\" + 0.003*\"one_favorit\" + 0.002*\"look_forward\" + '\n",
      "  '0.002*\"tv_show\" + 0.002*\"q\" + 0.002*\"ive_seen\"'),\n",
      " (8,\n",
      "  '0.005*\"foss\" + 0.004*\"lenni\" + 0.003*\"thoroughli_enjoy\" + '\n",
      "  '0.003*\"lenni_bruce\" + 0.002*\"stay_away\" + 0.002*\"jone\" + '\n",
      "  '0.002*\"bill_murray\" + 0.002*\"mulder\" + 0.002*\"first_half\" + '\n",
      "  '0.002*\"main_charact\"'),\n",
      " (9,\n",
      "  '0.007*\"holm\" + 0.004*\"well_written\" + 0.003*\"clown\" + 0.003*\"calam_jane\" + '\n",
      "  '0.003*\"well_worth\" + 0.003*\"filipino\" + 0.002*\"shave\" + 0.002*\"robin_hood\" '\n",
      "  '+ 0.002*\"come_togeth\" + 0.002*\"watson\"'),\n",
      " (10,\n",
      "  '0.005*\"film_festiv\" + 0.002*\"want_see\" + 0.002*\"wast_time\" + '\n",
      "  '0.002*\"ever_seen\" + 0.002*\"even_though\" + 0.002*\"bruce\" + 0.002*\"festiv\" + '\n",
      "  '0.002*\"definit_worth\" + 0.002*\"one_best\" + 0.002*\"stori_line\"'),\n",
      " (11,\n",
      "  '0.008*\"de_niro\" + 0.004*\"keira_knightley\" + 0.004*\"screamer\" + 0.003*\"niro\" '\n",
      "  '+ 0.002*\"main_charact\" + 0.002*\"african_american\" + 0.002*\"keira\" + '\n",
      "  '0.002*\"nightmar_elm\" + 0.002*\"ina\" + 0.002*\"knightley\"'),\n",
      " (12,\n",
      "  '0.005*\"south_africa\" + 0.003*\"attent_detail\" + 0.003*\"serial_killer\" + '\n",
      "  '0.003*\"10_10\" + 0.003*\"caesar\" + 0.003*\"zombi\" + 0.003*\"log\" + '\n",
      "  '0.003*\"ahead_time\" + 0.003*\"jason\" + 0.003*\"ever_made\"'),\n",
      " (13,\n",
      "  '0.003*\"episod\" + 0.003*\"uwe_boll\" + 0.003*\"well_done\" + 0.002*\"ive_seen\" + '\n",
      "  '0.002*\"adam\" + 0.002*\"star_war\" + 0.002*\"war\" + 0.002*\"special_effect\" + '\n",
      "  '0.002*\"want_see\" + 0.002*\"mother_sister\"'),\n",
      " (14,\n",
      "  '0.005*\"monk\" + 0.004*\"morgan_freeman\" + 0.003*\"lisa_kudrow\" + 0.002*\"plane\" '\n",
      "  '+ 0.002*\"far_better\" + 0.002*\"norway\" + 0.002*\"freeman\" + '\n",
      "  '0.002*\"one_favorit\" + 0.002*\"special_effect\" + 0.002*\"make_sens\"'),\n",
      " (15,\n",
      "  '0.002*\"one_favourit\" + 0.002*\"sound_like\" + 0.002*\"anyon_els\" + '\n",
      "  '0.002*\"tribe\" + 0.002*\"michel_yeoh\" + 0.002*\"90_minut\" + 0.002*\"along_way\" '\n",
      "  '+ 0.002*\"bad_act\" + 0.002*\"better\" + 0.002*\"what_go\"'),\n",
      " (16,\n",
      "  '0.009*\"werewolf\" + 0.006*\"philip_k\" + 0.004*\"ive_seen\" + 0.004*\"twist_end\" '\n",
      "  '+ 0.003*\"brought_back\" + 0.002*\"marin\" + 0.002*\"ive_never\" + '\n",
      "  '0.002*\"king_kong\" + 0.002*\"peter_weller\" + 0.002*\"k\"'),\n",
      " (17,\n",
      "  '0.003*\"brad_pitt\" + 0.002*\"wish_could\" + 0.002*\"put_togeth\" + '\n",
      "  '0.002*\"look_like\" + 0.002*\"marc\" + 0.002*\"15_minut\" + 0.002*\"main_charact\" '\n",
      "  '+ 0.002*\"top_notch\" + 0.002*\"littl_bit\" + 0.002*\"romant_comedi\"')]\n",
      "\n",
      "LSI\n",
      "[(0,\n",
      "  '0.100*\"special_effect\" + 0.085*\"look_like\" + 0.081*\"ive_seen\" + '\n",
      "  '0.080*\"one_best\" + 0.077*\"bad\" + 0.070*\"plot\" + 0.067*\"actor\" + '\n",
      "  '0.065*\"work\" + 0.065*\"main_charact\" + 0.065*\"say\"'),\n",
      " (1,\n",
      "  '-0.449*\"lenni_bruce\" + 0.420*\"special_effect\" + -0.304*\"lenni\" + '\n",
      "  '-0.242*\"dustin_hoffman\" + -0.233*\"bruce\" + -0.218*\"black_white\" + '\n",
      "  '-0.163*\"hoffman\" + -0.132*\"bob_foss\" + 0.127*\"special\" + '\n",
      "  '-0.120*\"valeri_perrin\"'),\n",
      " (2,\n",
      "  '-0.631*\"special_effect\" + -0.346*\"lenni_bruce\" + -0.224*\"lenni\" + '\n",
      "  '-0.184*\"dustin_hoffman\" + -0.184*\"special\" + -0.172*\"bruce\" + '\n",
      "  '-0.167*\"effect\" + -0.128*\"black_white\" + -0.123*\"hoffman\" + '\n",
      "  '-0.097*\"bob_foss\"'),\n",
      " (3,\n",
      "  '-0.508*\"beverli_hill\" + -0.225*\"eddi_murphi\" + -0.173*\"beverli\" + '\n",
      "  '-0.155*\"jacki_chan\" + -0.154*\"hill\" + -0.154*\"cop\" + 0.154*\"robin_hood\" + '\n",
      "  '-0.149*\"axel_foley\" + 0.141*\"bett_davi\" + -0.130*\"murphi\"'),\n",
      " (4,\n",
      "  '0.318*\"final_fantasi\" + -0.255*\"beverli_hill\" + 0.205*\"game\" + '\n",
      "  '0.203*\"ive_seen\" + -0.165*\"bett_davi\" + -0.164*\"special_effect\" + '\n",
      "  '-0.164*\"robin_hood\" + 0.163*\"ive_ever\" + -0.126*\"film_noir\" + '\n",
      "  '0.120*\"stori_line\"'),\n",
      " (5,\n",
      "  '0.411*\"jacki_chan\" + 0.299*\"robin_hood\" + 0.265*\"one_best\" + '\n",
      "  '0.238*\"ive_seen\" + 0.171*\"jacki\" + 0.168*\"robin\" + 0.166*\"chan\" + '\n",
      "  '0.155*\"errol_flynn\" + -0.137*\"main_charact\" + 0.127*\"ive_ever\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print('LDA')\n",
    "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=18, iterations=100)\n",
    "pprint(model.print_topics())\n",
    "\n",
    "print()\n",
    "print('LSI')\n",
    "\n",
    "model1 = LsiModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=6, onepass=True, power_iters=100)\n",
    "pprint(model1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"hoffman\" + 0.003*\"dont_miss\" + 0.003*\"monk\" + 0.003*\"90_minut\" + 0.002*\"main_charact\" + 0.002*\"one_greatest\" + 0.002*\"look_like\" + 0.002*\"pretti_good\" + 0.002*\"ginger\" + 0.002*\"even_though\"\n",
      "Topic: 1 Word: 0.006*\"we_anderson\" + 0.005*\"mother_sister\" + 0.003*\"act_superb\" + 0.003*\"definit_worth\" + 0.003*\"dustin\" + 0.003*\"lenni\" + 0.003*\"10_minut\" + 0.003*\"havent_seen\" + 0.003*\"video_store\" + 0.002*\"lenni_bruce\"\n",
      "Topic: 2 Word: 0.008*\"de_niro\" + 0.006*\"werewolf\" + 0.004*\"philip_k\" + 0.003*\"would_recommend\" + 0.003*\"camera_work\" + 0.003*\"niro\" + 0.003*\"real_life\" + 0.003*\"well_done\" + 0.003*\"norway\" + 0.002*\"tyron_power\"\n",
      "Topic: 3 Word: 0.005*\"morgan_freeman\" + 0.003*\"your_look\" + 0.003*\"fight_club\" + 0.003*\"freeman\" + 0.003*\"agatha_christi\" + 0.003*\"star_war\" + 0.002*\"one_best\" + 0.002*\"edg_seat\" + 0.002*\"nativ\" + 0.002*\"main_charact\"\n",
      "Topic: 4 Word: 0.003*\"king_kong\" + 0.002*\"ten_minut\" + 0.002*\"want_see\" + 0.002*\"make_sens\" + 0.002*\"reilli\" + 0.002*\"bad\" + 0.002*\"point_view\" + 0.002*\"children\" + 0.002*\"among_other\" + 0.002*\"youv_got\"\n",
      "Topic: 5 Word: 0.003*\"etc_etc\" + 0.003*\"one_best\" + 0.002*\"dan_futterman\" + 0.002*\"cant_wait\" + 0.002*\"episod\" + 0.002*\"film_maker\" + 0.002*\"cassidi\" + 0.002*\"year_ago\" + 0.002*\"walk_away\" + 0.002*\"much_better\"\n",
      "Topic: 6 Word: 0.005*\"brad_pitt\" + 0.004*\"christma\" + 0.003*\"plane\" + 0.003*\"ahead_time\" + 0.003*\"ginger_snap\" + 0.002*\"ginger\" + 0.002*\"bill_murray\" + 0.002*\"ava_gardner\" + 0.002*\"21st_centuri\" + 0.002*\"church\"\n",
      "Topic: 7 Word: 0.004*\"sherlock_holm\" + 0.004*\"must_see\" + 0.003*\"foss\" + 0.003*\"sherlock\" + 0.003*\"zombi\" + 0.003*\"ive_ever\" + 0.003*\"ive_seen\" + 0.002*\"marc\" + 0.002*\"tv_seri\" + 0.002*\"without_doubt\"\n",
      "Topic: 8 Word: 0.006*\"holm\" + 0.005*\"screamer\" + 0.003*\"romant_comedi\" + 0.003*\"twist_end\" + 0.002*\"action_flick\" + 0.002*\"entir_cast\" + 0.002*\"first_half\" + 0.002*\"despit_fact\" + 0.002*\"robin_hood\" + 0.002*\"cast\"\n",
      "Topic: 9 Word: 0.004*\"tunnel\" + 0.003*\"car_chase\" + 0.002*\"marin\" + 0.002*\"everi_singl\" + 0.002*\"read_review\" + 0.002*\"clinic\" + 0.002*\"show_us\" + 0.002*\"real_life\" + 0.002*\"twist_end\" + 0.002*\"whole_thing\"\n",
      "Topic: 10 Word: 0.004*\"look_forward\" + 0.003*\"anderson\" + 0.002*\"star_war\" + 0.002*\"one_favourit\" + 0.002*\"tv_show\" + 0.002*\"scienc_fiction\" + 0.002*\"tori\" + 0.002*\"plot_hole\" + 0.002*\"there_someth\" + 0.002*\"one_best\"\n",
      "Topic: 11 Word: 0.003*\"soap_opera\" + 0.003*\"tribe\" + 0.002*\"attent_detail\" + 0.002*\"stay_away\" + 0.002*\"five_minut\" + 0.002*\"walk_around\" + 0.002*\"havent_seen\" + 0.002*\"review\" + 0.002*\"one_best\" + 0.002*\"ha_ha\"\n",
      "Topic: 12 Word: 0.003*\"well_written\" + 0.003*\"juic\" + 0.002*\"base_true\" + 0.002*\"film_festiv\" + 0.002*\"old_man\" + 0.002*\"put_togeth\" + 0.002*\"nightmar_elm\" + 0.002*\"ive_seen\" + 0.002*\"modern_day\" + 0.002*\"muslim\"\n",
      "Topic: 13 Word: 0.003*\"moustach\" + 0.003*\"special_effect\" + 0.002*\"make_sens\" + 0.002*\"stori_line\" + 0.002*\"terrorist\" + 0.002*\"15_year\" + 0.002*\"7_10\" + 0.002*\"worth_watch\" + 0.002*\"took_place\" + 0.002*\"youv_got\"\n",
      "Topic: 14 Word: 0.004*\"indiana_jone\" + 0.003*\"south_africa\" + 0.003*\"lisa_kudrow\" + 0.003*\"michel_yeoh\" + 0.003*\"special_effect\" + 0.003*\"start_finish\" + 0.002*\"ive_seen\" + 0.002*\"bad_guy\" + 0.002*\"one_best\" + 0.002*\"log\"\n",
      "Topic: 15 Word: 0.004*\"serial_killer\" + 0.004*\"low_budget\" + 0.003*\"15_minut\" + 0.003*\"ive_seen\" + 0.002*\"bad_guy\" + 0.002*\"caesar\" + 0.002*\"highli_recommend\" + 0.002*\"christian_bale\" + 0.002*\"writer_director\" + 0.002*\"actor\"\n",
      "Topic: 16 Word: 0.004*\"robert_de\" + 0.004*\"keira_knightley\" + 0.003*\"adam\" + 0.003*\"ever_made\" + 0.002*\"billi_crystal\" + 0.002*\"one_worst\" + 0.002*\"good_job\" + 0.002*\"keira\" + 0.002*\"plot\" + 0.002*\"special_effect\"\n",
      "Topic: 17 Word: 0.003*\"uwe_boll\" + 0.003*\"look_like\" + 0.002*\"small_town\" + 0.002*\"joe_dant\" + 0.002*\"bill_murray\" + 0.002*\"first_saw\" + 0.002*\"boll\" + 0.002*\"10\" + 0.002*\"10_year\" + 0.002*\"1_10\"\n"
     ]
    }
   ],
   "source": [
    "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=18, iterations=100)\n",
    "model1 = LsiModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=6, onepass=True, power_iters=100)\n",
    "\n",
    "for idx, topic in model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 18)])\n",
    "\n",
    "#pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")\n",
    "df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_csv(\"top_words2.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
      "topic                                                \n",
      "13     0.099029  0.053613       1        1  24.643002\n",
      "17     0.045927 -0.005586       2        1  10.992599\n",
      "15     0.038881 -0.035311       3        1   9.106997\n",
      "14     0.030978 -0.084428       4        1   7.899704\n",
      "3      0.015495 -0.034375       5        1   6.660711\n",
      "1      0.036429  0.028984       6        1   6.179914\n",
      "10    -0.010607 -0.000022       7        1   3.982857\n",
      "11    -0.019669  0.014217       8        1   3.587323\n",
      "2     -0.007402  0.008293       9        1   3.558315\n",
      "5     -0.013541 -0.032758      10        1   3.252084\n",
      "6     -0.007907  0.021920      11        1   3.155763\n",
      "8     -0.043170  0.027007      12        1   3.065491\n",
      "0     -0.028183  0.006334      13        1   2.864668\n",
      "4     -0.008914  0.010435      14        1   2.800848\n",
      "7     -0.014697  0.032247      15        1   2.231612\n",
      "9     -0.038979 -0.003706      16        1   2.106798\n",
      "16    -0.030221 -0.003652      17        1   1.981936\n",
      "12    -0.043446 -0.003211      18        1   1.929375, topic_info=     Category       Freq              Term      Total  loglift  logprob\n",
      "term                                                                   \n",
      "7476  Default   7.000000           de_niro   7.000000  30.0000  30.0000\n",
      "393   Default  26.000000          ive_seen  26.000000  29.0000  29.0000\n",
      "5519  Default   5.000000          werewolf   5.000000  28.0000  28.0000\n",
      "1361  Default   9.000000              monk   9.000000  27.0000  27.0000\n",
      "1700  Default  13.000000       film_festiv  13.000000  26.0000  26.0000\n",
      "7491  Default   6.000000       bill_murray   6.000000  25.0000  25.0000\n",
      "426   Default  17.000000          uwe_boll  17.000000  24.0000  24.0000\n",
      "7387  Default   9.000000         king_kong   9.000000  23.0000  23.0000\n",
      "6367  Default   9.000000             lenni   9.000000  22.0000  22.0000\n",
      "5608  Default   5.000000              holm   5.000000  21.0000  21.0000\n",
      "2620  Default  14.000000          must_see  14.000000  20.0000  20.0000\n",
      "3737  Default   5.000000       we_anderson   5.000000  19.0000  19.0000\n",
      "139   Default  10.000000             zombi  10.000000  18.0000  18.0000\n",
      "7280  Default   6.000000     sherlock_holm   6.000000  17.0000  17.0000\n",
      "7381  Default   7.000000    morgan_freeman   7.000000  16.0000  16.0000\n",
      "1692  Default   6.000000      well_written   6.000000  15.0000  15.0000\n",
      "2988  Default  17.000000        stori_line  17.000000  14.0000  14.0000\n",
      "7546  Default   6.000000           hoffman   6.000000  13.0000  13.0000\n",
      "514   Default  12.000000      realli_enjoy  12.000000  12.0000  12.0000\n",
      "1148  Default  15.000000   would_recommend  15.000000  11.0000  11.0000\n",
      "519   Default  32.000000    special_effect  32.000000  10.0000  10.0000\n",
      "5625  Default  10.000000         brad_pitt  10.000000   9.0000   9.0000\n",
      "1704  Default  18.000000  highli_recommend  18.000000   8.0000   8.0000\n",
      "6644  Default   6.000000          christma   6.000000   7.0000   7.0000\n",
      "3059  Default   6.000000           tv_show   6.000000   6.0000   6.0000\n",
      "2535  Default  11.000000            dragon  11.000000   5.0000   5.0000\n",
      "7583  Default  13.000000       lenni_bruce  13.000000   4.0000   4.0000\n",
      "2403  Default  14.000000          star_war  14.000000   3.0000   3.0000\n",
      "7389  Default   4.000000         robert_de   4.000000   2.0000   2.0000\n",
      "1960  Default  16.000000          year_ago  16.000000   1.0000   1.0000\n",
      "...       ...        ...               ...        ...      ...      ...\n",
      "4404  Topic18   0.350681              mini   1.224923   2.6972  -7.1863\n",
      "6037  Topic18   0.278855         uncertain   0.974333   2.6969  -7.4155\n",
      "6073  Topic18   0.288310          stowaway   1.011846   2.6925  -7.3821\n",
      "6264  Topic18   0.326879         cameraman   1.163852   2.6781  -7.2566\n",
      "6653  Topic18   0.281772         tasteless   1.009163   2.6722  -7.4050\n",
      "2867  Topic18   0.327745         biographi   1.184026   2.6635  -7.2539\n",
      "7104  Topic18   0.276339        masterwork   1.009572   2.6523  -7.4245\n",
      "1909  Topic18   0.333159            tucker   1.222450   2.6480  -7.2375\n",
      "3749  Topic18   0.539827            serial   2.346635   2.4785  -6.7549\n",
      "3829  Topic18   0.395293            isabel   1.597934   2.5511  -7.0665\n",
      "139   Topic18   1.320760             zombi  10.740705   1.8521  -5.8602\n",
      "5129  Topic18   0.636730    facial_express   3.610659   2.2127  -6.5898\n",
      "3849  Topic18   0.779616         base_true   5.255279   2.0398  -6.3873\n",
      "7747  Topic18   0.854214         jason_lee   6.884351   1.8612  -6.2960\n",
      "206   Topic18   1.159451         ever_made  12.267937   1.5889  -5.9904\n",
      "514   Topic18   1.057451      realli_enjoy  12.004099   1.5186  -6.0825\n",
      "1408  Topic18   0.829133         anyth_els   7.598212   1.7327  -6.3258\n",
      "6757  Topic18   0.416744           lifetim   1.973485   2.3929  -7.0137\n",
      "1213  Topic18   0.645389      actress_play   5.307526   1.8409  -6.5763\n",
      "3051  Topic18   0.455287               sat   2.404759   2.2837  -6.9252\n",
      "1615  Topic18   0.782547      support_cast  10.313762   1.3693  -6.3836\n",
      "5819  Topic18   0.544440       ava_gardner   3.908908   1.9767  -6.7464\n",
      "2611  Topic18   0.621111      start_finish   8.730893   1.3049  -6.6146\n",
      "3673  Topic18   0.535334        act_superb   6.194279   1.4995  -6.7633\n",
      "2621  Topic18   0.545889           10_year   7.216142   1.3663  -6.7437\n",
      "456   Topic18   0.576011        moviebr_br  11.654688   0.9406  -6.6900\n",
      "6851  Topic18   0.549289           crystal   7.626994   1.3171  -6.7375\n",
      "1706  Topic18   0.564983          one_best  23.590635   0.2162  -6.7094\n",
      "2562  Topic18   0.550485       black_white  13.228800   0.7686  -6.7353\n",
      "7823  Topic18   0.543442     billi_crystal   9.028961   1.1377  -6.7482\n",
      "\n",
      "[1040 rows x 6 columns], token_table=      Topic      Freq            Term\n",
      "term                                 \n",
      "441       1  0.370393               1\n",
      "441       2  0.123464               1\n",
      "441      10  0.123464               1\n",
      "442       1  0.139689              10\n",
      "442       2  0.139689              10\n",
      "442       3  0.069845              10\n",
      "442       4  0.139689              10\n",
      "442       5  0.069845              10\n",
      "442       6  0.069845              10\n",
      "442       8  0.069845              10\n",
      "442      10  0.069845              10\n",
      "442      18  0.069845              10\n",
      "1900      2  0.226126           10_10\n",
      "1900     18  0.226126           10_10\n",
      "443       1  0.691922        10_minut\n",
      "443       2  0.098846        10_minut\n",
      "443       4  0.098846        10_minut\n",
      "443      10  0.098846        10_minut\n",
      "2621      3  0.415735         10_year\n",
      "2621      4  0.138578         10_year\n",
      "2621      8  0.138578         10_year\n",
      "2621     18  0.138578         10_year\n",
      "7211      2  0.817213        15_minut\n",
      "5581     11  0.707429            1968\n",
      "7181      8  0.800270            1981\n",
      "4054      5  0.251869            1_10\n",
      "4054      9  0.251869            1_10\n",
      "4054     10  0.503738            1_10\n",
      "2636      7  0.501868            2000\n",
      "6829      5  0.541908            2002\n",
      "...     ...       ...             ...\n",
      "1960     13  0.062142        year_ago\n",
      "1960     17  0.062142        year_ago\n",
      "1747      1  0.507474        year_old\n",
      "1747      2  0.112772        year_old\n",
      "1747      4  0.056386        year_old\n",
      "1747      5  0.056386        year_old\n",
      "1747      6  0.056386        year_old\n",
      "1747      7  0.056386        year_old\n",
      "1747      8  0.056386        year_old\n",
      "7033      3  0.760384            yeoh\n",
      "3981      4  0.305280       young_men\n",
      "3981      8  0.305280       young_men\n",
      "7678      1  0.166799  young_sherlock\n",
      "7678      3  0.166799  young_sherlock\n",
      "7678      4  0.166799  young_sherlock\n",
      "7678      9  0.166799  young_sherlock\n",
      "6467      1  0.365303       your_look\n",
      "6467      5  0.365303       your_look\n",
      "6467     15  0.121768       your_look\n",
      "5392      4  0.819272       youv_ever\n",
      "5097      1  0.264243        youv_got\n",
      "5097      2  0.396364        youv_got\n",
      "5097      5  0.132121        youv_got\n",
      "2648     12  0.358007       youv_seen\n",
      "2648     17  0.358007       youv_seen\n",
      "1819      5  0.526032               z\n",
      "139       1  0.558623           zombi\n",
      "139       2  0.093104           zombi\n",
      "139      13  0.093104           zombi\n",
      "139      18  0.093104           zombi\n",
      "\n",
      "[2055 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[14, 18, 16, 15, 4, 2, 11, 12, 3, 6, 7, 9, 1, 5, 8, 10, 17, 13])\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim;pyLDAvis.enable_notebook();\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(model, corpus_tfidf, dictionary)\n",
    "print(data)\n",
    "pyLDAvis.save_html(data, 'lda-gensim-18.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
